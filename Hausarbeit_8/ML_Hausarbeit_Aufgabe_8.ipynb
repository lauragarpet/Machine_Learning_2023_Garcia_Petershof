{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Hausarbeit 8\n",
        "\n",
        "• Modifizieren Sie das ResNet Beispiel aus dem Buch so, dass es dem\n",
        "Netzwerk aus dem Paper “Wide Residual Networks” von Zagoruyko et al.\n",
        "entspricht <br>\n",
        "• Verwenden Sie die gleichen Augmentation Methods wie in dem Paper\n",
        "(RandomHorizontalFlip und RandomCrop)<br>\n",
        "• Vergleichen Sie die Accuracy (Erkennungsraten) für Trainings- und\n",
        "Validierungsdaten mit dem Beispiel aus dem Buch<br>\n",
        "• Das Netzwerk dabei nur für die Klassifikation von Vögeln und Flugzeugen\n",
        "trainieren.<br>\n",
        "• Verwenden Sie 28 B(3,3) Blöcke mit k=2, [WRN-28-2-B(3,3)]<br>"
      ],
      "metadata": {
        "id": "QXK2rBazEGSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.Schritt: Laden der notwendigen Bibliotheken"
      ],
      "metadata": {
        "id": "QAqz-9x5ENkH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rmHHUNTgArw",
        "outputId": "b6f8302a-def0-4dcc-d1d5-108758a81e64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fdde0d312d0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Laden der notwendigen Bibliotheken\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import collections\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import datetime\n",
        "import torch\n",
        "import time\n",
        "import random\n",
        "\n",
        "torch.set_printoptions(edgeitems=2)\n",
        "torch.manual_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.Schritt: Laden der Bilddaten aus dem CIFAR10-Datensatz. Klassen: Vögel und Flugzeuge"
      ],
      "metadata": {
        "id": "xfOgwRtDEXdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition der einzelnen Klassen\n",
        "class_names_bird_plane = ['airplane','automobile','bird','cat','deer',\n",
        "               'dog','frog','horse','ship','truck']\n",
        "\n",
        "# Laden des Datensatzes: Umwandlung in einen Tensor und Normalisierung der Daten\n",
        "data_path = '../data-unversioned/p1ch6/'\n",
        "\n",
        "# Trainingsdatensatz\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                     (0.2470, 0.2435, 0.2616))\n",
        "    ]))\n",
        "\n",
        "# Validierungsdatensatz\n",
        "cifar10_val = datasets.CIFAR10(\n",
        "    data_path, train=False, download=True,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))\n",
        "\n",
        "# Bildung eines neuen Datensatzes, nur mit Bird und Flugzeug Bildern\n",
        "label_map = {0: 0, 2: 1}\n",
        "class_names_bird_plane = ['airplane', 'bird']\n",
        "\n",
        "# Trainingsdatensatz\n",
        "cifar2 = [(img, label_map[label])\n",
        "          for img, label in cifar10\n",
        "          if label in [0, 2]]\n",
        "\n",
        "# Validierungsdatensatz\n",
        "cifar2_val = [(img, label_map[label])\n",
        "              for img, label in cifar10_val\n",
        "              if label in [0, 2]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WItwjp86gDKx",
        "outputId": "3f75ce73-e58d-4226-c0bb-583051aed544"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.Schritt: Definition des ursprünglichen Modells mit dem 3x3-Kernel (Residual-Block aus dem Buch):\n"
      ],
      "metadata": {
        "id": "BXETtzl7EtIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition Residual-Block für ein neuronales Netzwerk\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, n_chans):\n",
        "\n",
        "        # Zur Initialisierung des RES-Blocks Pbjekts wird ein Konstruktor gebildet.\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        # 2D-Faltungsoperation mit einer 3x3 Kernel-Größe und einem Padding von 1.\n",
        "        # n_chans = Eingangskanäle und Ausgangskanäle\n",
        "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
        "                              padding=1, bias=False)\n",
        "\n",
        "        # Definition einer Batch-Normalisierung, die auf n_features angewendet wird.\n",
        "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
        "\n",
        "        # Gewichtung der Faltung mit der Kaiming-Normalisierung --> Abgestimmt auf die ReLU-Aktivierungsfunktion\n",
        "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
        "                                      nonlinearity='relu')\n",
        "\n",
        "        # Gewichte der Batch-Normalisierung werden auf den Wert 0.5 gesetzt.\n",
        "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
        "\n",
        "        # Biases der Batch-Normalisierung werden gleich 0 gesetzt.\n",
        "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
        "\n",
        "# Definition der Forward-Methode\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 1. Layer\n",
        "        # Der Eingabetensor x wird durch die Faltung (self.conv) geleitet,\n",
        "        # um eine Zwischenausgabe out zu erhalten.\n",
        "        out = self.conv(x)\n",
        "        # 2. Layer\n",
        "        # Die Zwischenausgabe out wird durch die Batch-Normalisierung\n",
        "        # (self.batch_norm) geleitet, um normierte Features zu erhalten.\n",
        "        out = self.batch_norm(out)\n",
        "        # 3. Layer\n",
        "        # Die normierten Features werden durch die ReLU-Aktivierungsfunktion\n",
        "        # geleitet (torch.relu).\n",
        "        out = torch.relu(out)\n",
        "\n",
        "        # Die Zwischenausgabe out wird mit dem Eingabetensor x addiert,\n",
        "        # um den Residual-Verbindung zu implementieren.\n",
        "        return out + x\n",
        "\n",
        "# Definition des neuronalen Netzwerks mit Residualblöcken\n",
        "class NetResDeep(nn.Module):\n",
        "\n",
        "    def __init__(self, n_chans1=32, n_blocks=10):\n",
        "\n",
        "        # Konstrukur, um das NetResDeep-Objekt zu initialisieren\n",
        "        super().__init__()\n",
        "\n",
        "        # Festlegung der Kanäle (Channels), die in der ersten Faltungsschicht verwendet werden sollen.\n",
        "        # Hier = 32\n",
        "        self.n_chans1 = n_chans1\n",
        "\n",
        "        # Definition einer 2D-Faltungsschicht, die einen Eingabetensor mit 3 Kanälen (RGB-Bild) nimmt und\n",
        "        # n_channel Ausgangskanäle erzeugt.\n",
        "        # Kernel: 3x3 und Padding: 1\n",
        "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
        "\n",
        "        # Definition einer Sequenz von Residual-Blöcken\n",
        "        # Die Sequenz: es wird n_blocks-mal ein Residualblock mit n_chans1 Kanälen\n",
        "        # erstellt und in die Sequenz eingefügt.\n",
        "        self.resblocks = nn.Sequential(\n",
        "            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
        "\n",
        "        # Definition eines vollständig verbundenen Layers\n",
        "        # Unter der Annahme, dass der Input nach dem Pooling 8x8 groß ist.\n",
        "        # Ausgabevektor ist danach 32-Groß\n",
        "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
        "\n",
        "        # Definition zweiter vollständig verbundener Layer\n",
        "        # Eingabevektor = 32 und Ausgabevektor = 2\n",
        "        self.fc2 = nn.Linear(32, 2)\n",
        "\n",
        "# Definition der Forward-Methode\n",
        "    def forward(self, x):\n",
        "        # Der Eingabetensor x wird durch die erste Faltung (self.conv1)\n",
        "        # geleitet und dann mit der ReLU-Aktivierungsfunktion und einer\n",
        "        # 2x2-Max-Pooling-Operation verarbeitet.\n",
        "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
        "\n",
        "        # Die verarbeiteten Features werden durch die Sequenz von\n",
        "        # Residualblöcken (self.resblocks) geleitet.\n",
        "        out = self.resblocks(out)\n",
        "\n",
        "        # Die resultierenden Features werden erneut mit einer\n",
        "        # 2x2-Max-Pooling-Operation verarbeitet.\n",
        "        out = F.max_pool2d(out, 2)\n",
        "\n",
        "        # Die resultierenden Features werden in einen Vektor umgeformt\n",
        "        # (out.view(-1, 8 * 8 * self.n_chans1)), um sie für\n",
        "        # den vollständig verbundenen Layer vorzubereiten.\n",
        "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
        "\n",
        "        # Der umgeformte Vektor wird durch den ersten vollständig verbundenen\n",
        "        # Layer (self.fc1) mit ReLU-Aktivierung geleitet.\n",
        "        out = torch.relu(self.fc1(out))\n",
        "\n",
        "        # Die resultierenden Features werden durch den zweiten vollständig\n",
        "        # verbundenen Layer (self.fc2) geleitet, um den endgültigen Ausgabevektor zu erhalten.\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        # Der Ausgabevektor wird zurückgegeben.\n",
        "        return out\n",
        "\n",
        "# Definition des Trainingloops\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device)  # <1>\n",
        "            labels = labels.to(device=device)\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % 1 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))\n",
        "\n",
        "# Trainieren des Modells\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "model_Net_Res = NetResDeep(n_chans1=32, n_blocks=10).to(device=device)\n",
        "optimizer = optim.SGD(model_Net_Res.parameters(), lr=1e-2)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 100,\n",
        "    optimizer = optimizer,\n",
        "    model = model_Net_Res,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "\n",
        "# Berechnung der Genauigkeit des Modells für die Trainings und Validierungsdaten:\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "all_acc_dict = collections.OrderedDict()\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    accdict = {}\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device=device)  # <1>\n",
        "                labels = labels.to(device=device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1) # <1>\n",
        "                total += labels.shape[0]\n",
        "                correct += int((predicted == labels).sum())\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "\n",
        "\n",
        "validate(model_Net_Res, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8NRYDwvEy1T",
        "outputId": "bf1aa45e-c045-4912-97fb-a6aa71b39ae6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-19 11:30:40.244843 Epoch 1, Training loss 0.5254586413977252\n",
            "2023-06-19 11:30:41.067804 Epoch 2, Training loss 0.3711422786211512\n",
            "2023-06-19 11:30:41.892674 Epoch 3, Training loss 0.33534020555626814\n",
            "2023-06-19 11:30:42.708603 Epoch 4, Training loss 0.30953430104407537\n",
            "2023-06-19 11:30:43.525190 Epoch 5, Training loss 0.29186353590458064\n",
            "2023-06-19 11:30:44.345746 Epoch 6, Training loss 0.2817312352311839\n",
            "2023-06-19 11:30:45.187596 Epoch 7, Training loss 0.26892885328478117\n",
            "2023-06-19 11:30:46.004616 Epoch 8, Training loss 0.25347367583946057\n",
            "2023-06-19 11:30:46.809688 Epoch 9, Training loss 0.24761364879501854\n",
            "2023-06-19 11:30:47.632964 Epoch 10, Training loss 0.23509505001982306\n",
            "2023-06-19 11:30:48.462002 Epoch 11, Training loss 0.22021974794044616\n",
            "2023-06-19 11:30:49.283427 Epoch 12, Training loss 0.20918062580808713\n",
            "2023-06-19 11:30:50.232600 Epoch 13, Training loss 0.20579675703671327\n",
            "2023-06-19 11:30:51.180515 Epoch 14, Training loss 0.18839915163198095\n",
            "2023-06-19 11:30:52.006137 Epoch 15, Training loss 0.18211028354752595\n",
            "2023-06-19 11:30:52.830467 Epoch 16, Training loss 0.18306673261200546\n",
            "2023-06-19 11:30:53.652918 Epoch 17, Training loss 0.15963327580956138\n",
            "2023-06-19 11:30:54.466561 Epoch 18, Training loss 0.14946642767187138\n",
            "2023-06-19 11:30:55.286555 Epoch 19, Training loss 0.1369413146689819\n",
            "2023-06-19 11:30:56.108025 Epoch 20, Training loss 0.14005746198877408\n",
            "2023-06-19 11:30:56.918628 Epoch 21, Training loss 0.12502556386741864\n",
            "2023-06-19 11:30:57.771242 Epoch 22, Training loss 0.11135911763568593\n",
            "2023-06-19 11:30:58.619126 Epoch 23, Training loss 0.1157104435975954\n",
            "2023-06-19 11:30:59.426540 Epoch 24, Training loss 0.09660001374354028\n",
            "2023-06-19 11:31:00.230344 Epoch 25, Training loss 0.0879445412807214\n",
            "2023-06-19 11:31:01.034078 Epoch 26, Training loss 0.09397054441083388\n",
            "2023-06-19 11:31:01.974808 Epoch 27, Training loss 0.09202883118514422\n",
            "2023-06-19 11:31:02.906432 Epoch 28, Training loss 0.07577211600224114\n",
            "2023-06-19 11:31:03.773765 Epoch 29, Training loss 0.08590198205012804\n",
            "2023-06-19 11:31:04.611198 Epoch 30, Training loss 0.08998521759062056\n",
            "2023-06-19 11:31:05.451065 Epoch 31, Training loss 0.049421759001958145\n",
            "2023-06-19 11:31:06.277978 Epoch 32, Training loss 0.03664286717013189\n",
            "2023-06-19 11:31:07.084709 Epoch 33, Training loss 0.04427734970346473\n",
            "2023-06-19 11:31:07.900252 Epoch 34, Training loss 0.02481304130748294\n",
            "2023-06-19 11:31:08.722039 Epoch 35, Training loss 0.022068751811281227\n",
            "2023-06-19 11:31:09.551547 Epoch 36, Training loss 0.021494055011125793\n",
            "2023-06-19 11:31:10.360826 Epoch 37, Training loss 0.019313514664187837\n",
            "2023-06-19 11:31:11.185129 Epoch 38, Training loss 0.01385008993300544\n",
            "2023-06-19 11:31:12.020773 Epoch 39, Training loss 0.09570695939592448\n",
            "2023-06-19 11:31:12.867547 Epoch 40, Training loss 0.1314531550713026\n",
            "2023-06-19 11:31:13.720972 Epoch 41, Training loss 0.07759091076530089\n",
            "2023-06-19 11:31:14.575757 Epoch 42, Training loss 0.05293257112146183\n",
            "2023-06-19 11:31:15.401009 Epoch 43, Training loss 0.07609971714425523\n",
            "2023-06-19 11:31:16.236323 Epoch 44, Training loss 0.09179330292412904\n",
            "2023-06-19 11:31:17.070088 Epoch 45, Training loss 0.10697469169831579\n",
            "2023-06-19 11:31:17.881987 Epoch 46, Training loss 0.06608209309945251\n",
            "2023-06-19 11:31:18.723400 Epoch 47, Training loss 0.04321413267712305\n",
            "2023-06-19 11:31:19.542439 Epoch 48, Training loss 0.030069950389658\n",
            "2023-06-19 11:31:20.349197 Epoch 49, Training loss 0.02298016633221488\n",
            "2023-06-19 11:31:21.174229 Epoch 50, Training loss 0.03335773522200383\n",
            "2023-06-19 11:31:21.993925 Epoch 51, Training loss 0.014929413513701622\n",
            "2023-06-19 11:31:22.798297 Epoch 52, Training loss 0.01478285058312545\n",
            "2023-06-19 11:31:23.601751 Epoch 53, Training loss 0.011767556560136805\n",
            "2023-06-19 11:31:24.462428 Epoch 54, Training loss 0.009113957263102197\n",
            "2023-06-19 11:31:25.407836 Epoch 55, Training loss 0.007756748040403969\n",
            "2023-06-19 11:31:26.277629 Epoch 56, Training loss 0.006247829094104273\n",
            "2023-06-19 11:31:27.091581 Epoch 57, Training loss 0.15014656030448378\n",
            "2023-06-19 11:31:27.905842 Epoch 58, Training loss 0.08395800694740217\n",
            "2023-06-19 11:31:28.738221 Epoch 59, Training loss 0.0855737567633082\n",
            "2023-06-19 11:31:29.569334 Epoch 60, Training loss 0.05448092181881768\n",
            "2023-06-19 11:31:30.382111 Epoch 61, Training loss 0.04952353017091763\n",
            "2023-06-19 11:31:31.192674 Epoch 62, Training loss 0.049708320949060525\n",
            "2023-06-19 11:31:32.011967 Epoch 63, Training loss 0.043383290503280846\n",
            "2023-06-19 11:31:32.814921 Epoch 64, Training loss 0.1391473797321984\n",
            "2023-06-19 11:31:33.626313 Epoch 65, Training loss 0.055378956414735434\n",
            "2023-06-19 11:31:34.456963 Epoch 66, Training loss 0.034643177963366176\n",
            "2023-06-19 11:31:35.284160 Epoch 67, Training loss 0.07942252549947874\n",
            "2023-06-19 11:31:36.146751 Epoch 68, Training loss 0.04486537233712804\n",
            "2023-06-19 11:31:36.982579 Epoch 69, Training loss 0.041184512337938156\n",
            "2023-06-19 11:31:37.836281 Epoch 70, Training loss 0.029894515812681738\n",
            "2023-06-19 11:31:38.662284 Epoch 71, Training loss 0.02122993322009974\n",
            "2023-06-19 11:31:39.476363 Epoch 72, Training loss 0.044793244260348454\n",
            "2023-06-19 11:31:40.295460 Epoch 73, Training loss 0.043981389360573546\n",
            "2023-06-19 11:31:41.108758 Epoch 74, Training loss 0.019908941645993834\n",
            "2023-06-19 11:31:41.927801 Epoch 75, Training loss 0.015324040990994625\n",
            "2023-06-19 11:31:42.732574 Epoch 76, Training loss 0.016989099143583115\n",
            "2023-06-19 11:31:43.530459 Epoch 77, Training loss 0.012065845640646116\n",
            "2023-06-19 11:31:44.336000 Epoch 78, Training loss 0.008761877796329427\n",
            "2023-06-19 11:31:45.149333 Epoch 79, Training loss 0.008216027362653274\n",
            "2023-06-19 11:31:45.973555 Epoch 80, Training loss 0.09894328585223022\n",
            "2023-06-19 11:31:46.800689 Epoch 81, Training loss 0.07500412087860832\n",
            "2023-06-19 11:31:47.671456 Epoch 82, Training loss 0.01707607387212123\n",
            "2023-06-19 11:31:48.526912 Epoch 83, Training loss 0.014758599284001193\n",
            "2023-06-19 11:31:49.371612 Epoch 84, Training loss 0.019071413081726593\n",
            "2023-06-19 11:31:50.189194 Epoch 85, Training loss 0.00803571893532529\n",
            "2023-06-19 11:31:51.016786 Epoch 86, Training loss 0.007356772500321649\n",
            "2023-06-19 11:31:51.860235 Epoch 87, Training loss 0.0064768503871627725\n",
            "2023-06-19 11:31:52.664774 Epoch 88, Training loss 0.005579213698722339\n",
            "2023-06-19 11:31:53.474307 Epoch 89, Training loss 0.005113721553330416\n",
            "2023-06-19 11:31:54.292469 Epoch 90, Training loss 0.005230793284154408\n",
            "2023-06-19 11:31:55.097710 Epoch 91, Training loss 0.004691144412292233\n",
            "2023-06-19 11:31:55.912071 Epoch 92, Training loss 0.004469909318703783\n",
            "2023-06-19 11:31:56.724816 Epoch 93, Training loss 0.00461209995382366\n",
            "2023-06-19 11:31:57.553737 Epoch 94, Training loss 0.0043204545151296955\n",
            "2023-06-19 11:31:58.372853 Epoch 95, Training loss 0.004546519695019003\n",
            "2023-06-19 11:31:59.224853 Epoch 96, Training loss 0.22775042030343395\n",
            "2023-06-19 11:32:00.151324 Epoch 97, Training loss 0.03258252327010083\n",
            "2023-06-19 11:32:00.998334 Epoch 98, Training loss 0.02280096728710612\n",
            "2023-06-19 11:32:01.819573 Epoch 99, Training loss 0.0545698508092315\n",
            "2023-06-19 11:32:02.674752 Epoch 100, Training loss 0.02607717502092539\n",
            "Accuracy train: 1.00\n",
            "Accuracy val: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.Schritt: WRN: Wide Residual Network\n",
        "\n",
        "**Anforderungen an das Netzwerk:**\n",
        "- Augmentation Methoden: RandomHorizontalFlip und RandomCrop\n",
        "\n",
        "- Sie 28 B(3,3) Layers mit k=2, [WRN-28-2-B(3,3)]\n"
      ],
      "metadata": {
        "id": "pFb4LpWdE5uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definition des Residual-Blocks für das neuronale Netzwerk\n",
        "class WRN_Block(nn.Module):\n",
        "    # Übergabe der Anzahl der Channels und des Strides\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(WRN_Block, self).__init__()\n",
        "        # Batch-Normalisierung der Anzahl der in_channels\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        # ReLU-Aktivierungsfunktion\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        # 1. Convolutional-Layer im Block.\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        # Batch-Normalisierung definiert mit der Anzahl der out_channels\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        # RelU-Aktivierungsfunktion\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        # 2. Convolutional-Layer im Block\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        # Anpassung, falls in_channels != out_channels\n",
        "        self.adjust = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),)\n",
        "\n",
        "        torch.nn.init.kaiming_normal_(self.conv1.weight,\n",
        "                                      nonlinearity='relu')\n",
        "        torch.nn.init.kaiming_normal_(self.conv2.weight,\n",
        "                                      nonlinearity='relu')\n",
        "\n",
        "        torch.nn.init.constant_(self.bn1.weight, 0.5)\n",
        "        torch.nn.init.constant_(self.bn2.weight, 0.5)\n",
        "\n",
        "        torch.nn.init.constant_(self.bn1.bias, 0.0)\n",
        "        torch.nn.init.constant_(self.bn2.bias, 0.0)\n",
        "\n",
        "    # Definition der Forward-Methode\n",
        "    def forward(self, input):\n",
        "        #input = x\n",
        "\n",
        "        # Anwendung der ersten Batch-Normalisierung\n",
        "        out = self.bn1(input)\n",
        "        # Anwendung der ersten ReLU-Aktivierungsfunktion\n",
        "        out = self.relu1(out)\n",
        "        # Anwendung des ersten Convolutional Layers\n",
        "        out = self.conv1(out)\n",
        "        # Anwendung der zweiten Batch-Normalisierung\n",
        "        out = self.bn2(out)\n",
        "        # Anwendung der zweiten ReLU-Aktivierungsfuntkion\n",
        "        out = self.relu2(out)\n",
        "        # Anwendung des zweiten Convolutional Layers\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        # Anpassung der Eingabe, sodass zwischen den Blöcken keine Probleme mit unterschiedlich großen Eingabewerten entsteht.\n",
        "        if input.size() != out.size():\n",
        "            # Anwendung des zusätzlichen Convolutional Layers, falls die Dimensionen zwischen den Blöcken nicht passen.\n",
        "            input = self.adjust(input)\n",
        "\n",
        "        return out + input\n",
        "\n",
        "# Definition des künstlichen neuronalen Netzes\n",
        "class WRN(nn.Module):\n",
        "    # Übergabe der Anzahl der Blöcke, der Weite der Layers und die Anzahl der vorhandenen Klassen\n",
        "    def __init__(self, num_blocks=4, k=2, num_classes=2):\n",
        "        super(WRN, self).__init__()\n",
        "        # Anzahl der input-Channels\n",
        "        self.in_channels = 16\n",
        "        # Definition des ersten Convolutional-Layers vor den Residual-Blöcken (convolutional group 1: 16)\n",
        "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        # Definition der ReLU-Aktivierungsfunktion\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        ### Definition der Übergabeparameter für die Blöcke\n",
        "        # Convolutional group 2: 16*k, num_blocks: Anzahl von Blöcken innerhalb der Gruppe.\n",
        "        self.group_conv2 = self.block(16*k, num_blocks)\n",
        "        # Convolutional group 3: 32*k\n",
        "        self.group_conv3 = self.block(32*k, num_blocks, stride=2)\n",
        "        # Convolutional group 4: 64*k\n",
        "        self.group_conv4 = self.block(64*k, num_blocks, stride=2)\n",
        "\n",
        "        # Definition der Batch-Normalisierung\n",
        "        self.bn = nn.BatchNorm2d(64*k)\n",
        "        # Definition des Average Poolings\n",
        "        self.avg_pool = nn.AvgPool2d(8, stride=1)\n",
        "        # Definition des Linearisierungsfunktion, mit 64*k Neuronen, die zu num_classes = 2 transferiert werden.\n",
        "        self.fc = nn.Linear(64*k, num_classes)\n",
        "\n",
        "    # Erzeugung einer Sequenz aus mehreren (num_blocks) Residual Blöcken\n",
        "    # Übergabe der Anzahl der Output-Channels und der Anzahl der N-Residual Blöcken\n",
        "    def block(self, out_channels, num_blocks=4, stride=1):\n",
        "        # Leere Liste für die einzelnen Residual-Blöcke\n",
        "        layers = []\n",
        "        layers.append(WRN_Block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(WRN_Block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "        # Die Sequenz enthält alle erstellten Wide Residual Blocks, die in der richtigen Reihenfolge\n",
        "        # ausgeführt werden, wenn der forward-Methode des WideResidualNetworks aufgerufen wird.\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        input = x\n",
        "        # 1. Convolutional-Layer vor den Blöcken\n",
        "        out_group_conv1 = self.conv1(input)\n",
        "        # 2. Gruppe: Conv2 - 16*k\n",
        "        out_group_conv2 = self.group_conv2(out_group_conv1)\n",
        "        # 3. Gruppe: Conv3 - 32*k\n",
        "        out_group_conv3 = self.group_conv3(out_group_conv2)\n",
        "        # 4. Gruppe: Conv4 - 64*k\n",
        "        out_group_conv4 = self.group_conv4(out_group_conv3)\n",
        "\n",
        "        # Anwendung der Batch-Normalisierung auf das Ergebnis nach den Blöcken\n",
        "        out = self.bn(out_group_conv4)\n",
        "        # Anwendung der ReLU-Aktivierungsfunktion\n",
        "        out = self.relu(out)\n",
        "        # Anwendung des Avg-Poolings\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        # Anwendung der Linearisierungsfunktion\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device)  # <1>\n",
        "            labels = labels.to(device=device)\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % 1 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))\n",
        "\n",
        "device = torch.device('cuda')\n",
        "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
        "all_acc_dict = collections.OrderedDict()\n",
        "\n",
        "model = WRN().to(device=device)\n",
        "lr = 0.001\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 200,\n",
        "    optimizer = optimizer,\n",
        "    model = model,\n",
        "    loss_fn = loss_fn,\n",
        "    train_loader = train_loader,\n",
        ")\n",
        "\n",
        "def validate(model, train_loader, val_loader):\n",
        "    accdict = {}\n",
        "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs = imgs.to(device=device)  # <1>\n",
        "                labels = labels.to(device=device)\n",
        "                outputs = model(imgs)\n",
        "                _, predicted = torch.max(outputs, dim=1) # <1>\n",
        "                total += labels.shape[0]\n",
        "                correct += int((predicted == labels).sum())\n",
        "\n",
        "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
        "        accdict[name] = correct / total\n",
        "    return accdict\n",
        "\n",
        "print(\"Number of free parameters in the model:\",sum(p.numel() for p in model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNwd5fiIgbhy",
        "outputId": "0cf11d1a-fdbd-4bf2-aa46-779322743133"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-19 10:14:21.289097 Epoch 1, Training loss 0.6734805722145518\n",
            "2023-06-19 10:14:23.468085 Epoch 2, Training loss 0.6095718187131699\n",
            "2023-06-19 10:14:25.680898 Epoch 3, Training loss 0.571550208102366\n",
            "2023-06-19 10:14:27.854666 Epoch 4, Training loss 0.5420037718715182\n",
            "2023-06-19 10:14:30.038029 Epoch 5, Training loss 0.5232469234496925\n",
            "2023-06-19 10:14:32.211222 Epoch 6, Training loss 0.5099179317617113\n",
            "2023-06-19 10:14:34.363320 Epoch 7, Training loss 0.49577421348565703\n",
            "2023-06-19 10:14:36.588148 Epoch 8, Training loss 0.4839676960258727\n",
            "2023-06-19 10:14:38.750957 Epoch 9, Training loss 0.47254138273797974\n",
            "2023-06-19 10:14:40.917973 Epoch 10, Training loss 0.4627764114908352\n",
            "2023-06-19 10:14:43.081495 Epoch 11, Training loss 0.4540910512019115\n",
            "2023-06-19 10:14:45.231417 Epoch 12, Training loss 0.4455526864073079\n",
            "2023-06-19 10:14:47.431838 Epoch 13, Training loss 0.4413351940501268\n",
            "2023-06-19 10:14:49.627835 Epoch 14, Training loss 0.4330986434487021\n",
            "2023-06-19 10:14:51.819093 Epoch 15, Training loss 0.42918391345412865\n",
            "2023-06-19 10:14:53.983196 Epoch 16, Training loss 0.42303543238882807\n",
            "2023-06-19 10:14:56.153197 Epoch 17, Training loss 0.4193669157043384\n",
            "2023-06-19 10:14:58.321927 Epoch 18, Training loss 0.4169533184379529\n",
            "2023-06-19 10:15:00.537957 Epoch 19, Training loss 0.4127945628515474\n",
            "2023-06-19 10:15:02.721368 Epoch 20, Training loss 0.40836438156996563\n",
            "2023-06-19 10:15:04.940773 Epoch 21, Training loss 0.4043780772169684\n",
            "2023-06-19 10:15:07.111986 Epoch 22, Training loss 0.4033472968894205\n",
            "2023-06-19 10:15:09.276066 Epoch 23, Training loss 0.3981997229301246\n",
            "2023-06-19 10:15:11.454568 Epoch 24, Training loss 0.39559471597717066\n",
            "2023-06-19 10:15:13.607678 Epoch 25, Training loss 0.3908622119646923\n",
            "2023-06-19 10:15:15.768292 Epoch 26, Training loss 0.38673859322147003\n",
            "2023-06-19 10:15:17.927174 Epoch 27, Training loss 0.38354087255562946\n",
            "2023-06-19 10:15:20.093363 Epoch 28, Training loss 0.3835767208580758\n",
            "2023-06-19 10:15:22.307072 Epoch 29, Training loss 0.38048414942944886\n",
            "2023-06-19 10:15:24.449833 Epoch 30, Training loss 0.3749741574951038\n",
            "2023-06-19 10:15:26.600199 Epoch 31, Training loss 0.3719120707101883\n",
            "2023-06-19 10:15:28.752058 Epoch 32, Training loss 0.37092750884924724\n",
            "2023-06-19 10:15:30.914932 Epoch 33, Training loss 0.36657610488165715\n",
            "2023-06-19 10:15:33.111683 Epoch 34, Training loss 0.3665293409566211\n",
            "2023-06-19 10:15:35.320180 Epoch 35, Training loss 0.3647860624607961\n",
            "2023-06-19 10:15:37.485421 Epoch 36, Training loss 0.3586097589343976\n",
            "2023-06-19 10:15:39.651739 Epoch 37, Training loss 0.35464688547097956\n",
            "2023-06-19 10:15:41.825484 Epoch 38, Training loss 0.3564899099670398\n",
            "2023-06-19 10:15:44.006977 Epoch 39, Training loss 0.35321677243633637\n",
            "2023-06-19 10:15:46.246255 Epoch 40, Training loss 0.34992768610738645\n",
            "2023-06-19 10:15:48.404354 Epoch 41, Training loss 0.34720468748906613\n",
            "2023-06-19 10:15:50.572373 Epoch 42, Training loss 0.3492393215560609\n",
            "2023-06-19 10:15:52.737624 Epoch 43, Training loss 0.34196746842876363\n",
            "2023-06-19 10:15:54.892438 Epoch 44, Training loss 0.33858129230274514\n",
            "2023-06-19 10:15:57.149078 Epoch 45, Training loss 0.33780159056186676\n",
            "2023-06-19 10:15:59.312509 Epoch 46, Training loss 0.332996269794786\n",
            "2023-06-19 10:16:01.486520 Epoch 47, Training loss 0.3341827672568096\n",
            "2023-06-19 10:16:03.741799 Epoch 48, Training loss 0.3314984878347178\n",
            "2023-06-19 10:16:05.908214 Epoch 49, Training loss 0.3281723333012526\n",
            "2023-06-19 10:16:08.146659 Epoch 50, Training loss 0.32401803325695594\n",
            "2023-06-19 10:16:10.332580 Epoch 51, Training loss 0.32219365362528785\n",
            "2023-06-19 10:16:12.518454 Epoch 52, Training loss 0.31951888304227477\n",
            "2023-06-19 10:16:14.670799 Epoch 53, Training loss 0.3187524355520868\n",
            "2023-06-19 10:16:16.846699 Epoch 54, Training loss 0.3222817953225154\n",
            "2023-06-19 10:16:19.041051 Epoch 55, Training loss 0.3153456704821556\n",
            "2023-06-19 10:16:21.229704 Epoch 56, Training loss 0.3115790216786087\n",
            "2023-06-19 10:16:23.384757 Epoch 57, Training loss 0.3106484546023569\n",
            "2023-06-19 10:16:25.536733 Epoch 58, Training loss 0.3074039603304711\n",
            "2023-06-19 10:16:27.699405 Epoch 59, Training loss 0.31166054564676465\n",
            "2023-06-19 10:16:29.880712 Epoch 60, Training loss 0.3018422312797255\n",
            "2023-06-19 10:16:32.090055 Epoch 61, Training loss 0.30378768170715137\n",
            "2023-06-19 10:16:34.241865 Epoch 62, Training loss 0.3042744251952809\n",
            "2023-06-19 10:16:36.393254 Epoch 63, Training loss 0.30144274747295746\n",
            "2023-06-19 10:16:38.561844 Epoch 64, Training loss 0.29814075569438325\n",
            "2023-06-19 10:16:40.728706 Epoch 65, Training loss 0.29660660085404755\n",
            "2023-06-19 10:16:42.928770 Epoch 66, Training loss 0.2925844066272116\n",
            "2023-06-19 10:16:45.087136 Epoch 67, Training loss 0.29209535146594806\n",
            "2023-06-19 10:16:47.237246 Epoch 68, Training loss 0.29268572778458807\n",
            "2023-06-19 10:16:49.399935 Epoch 69, Training loss 0.2866822208758372\n",
            "2023-06-19 10:16:51.553299 Epoch 70, Training loss 0.2856642152568337\n",
            "2023-06-19 10:16:53.745891 Epoch 71, Training loss 0.28486743673777126\n",
            "2023-06-19 10:16:55.923129 Epoch 72, Training loss 0.2849165246744824\n",
            "2023-06-19 10:16:58.084428 Epoch 73, Training loss 0.2820632519900419\n",
            "2023-06-19 10:17:00.265343 Epoch 74, Training loss 0.2768155545186085\n",
            "2023-06-19 10:17:02.481287 Epoch 75, Training loss 0.27615211306104237\n",
            "2023-06-19 10:17:04.706840 Epoch 76, Training loss 0.27575508562053086\n",
            "2023-06-19 10:17:06.904891 Epoch 77, Training loss 0.27270029812671576\n",
            "2023-06-19 10:17:09.070490 Epoch 78, Training loss 0.2724453839147167\n",
            "2023-06-19 10:17:11.239036 Epoch 79, Training loss 0.26952746657619053\n",
            "2023-06-19 10:17:13.398310 Epoch 80, Training loss 0.2642095831169444\n",
            "2023-06-19 10:17:15.560712 Epoch 81, Training loss 0.2663111763110586\n",
            "2023-06-19 10:17:17.743981 Epoch 82, Training loss 0.26795128756647657\n",
            "2023-06-19 10:17:19.907739 Epoch 83, Training loss 0.2618026291583754\n",
            "2023-06-19 10:17:22.079057 Epoch 84, Training loss 0.25978795500697605\n",
            "2023-06-19 10:17:24.243801 Epoch 85, Training loss 0.259868695572683\n",
            "2023-06-19 10:17:26.408714 Epoch 86, Training loss 0.2564336599627878\n",
            "2023-06-19 10:17:28.631574 Epoch 87, Training loss 0.2512882677422967\n",
            "2023-06-19 10:17:30.821414 Epoch 88, Training loss 0.2541928934823176\n",
            "2023-06-19 10:17:32.973818 Epoch 89, Training loss 0.25084508518884135\n",
            "2023-06-19 10:17:35.121909 Epoch 90, Training loss 0.24693644151186486\n",
            "2023-06-19 10:17:37.287383 Epoch 91, Training loss 0.246993344585607\n",
            "2023-06-19 10:17:39.518941 Epoch 92, Training loss 0.2460960067191701\n",
            "2023-06-19 10:17:41.722787 Epoch 93, Training loss 0.24469450201578202\n",
            "2023-06-19 10:17:43.891096 Epoch 94, Training loss 0.23937519117715253\n",
            "2023-06-19 10:17:46.066580 Epoch 95, Training loss 0.23903933309825362\n",
            "2023-06-19 10:17:48.234400 Epoch 96, Training loss 0.23478374098706398\n",
            "2023-06-19 10:17:50.414842 Epoch 97, Training loss 0.23799571247806975\n",
            "2023-06-19 10:17:52.668187 Epoch 98, Training loss 0.23207937280653387\n",
            "2023-06-19 10:17:54.834045 Epoch 99, Training loss 0.23021058937546554\n",
            "2023-06-19 10:17:57.008544 Epoch 100, Training loss 0.22758810782128838\n",
            "2023-06-19 10:17:59.156041 Epoch 101, Training loss 0.22782038778636107\n",
            "2023-06-19 10:18:01.317644 Epoch 102, Training loss 0.22277111887552176\n",
            "2023-06-19 10:18:03.640671 Epoch 103, Training loss 0.22147867520144032\n",
            "2023-06-19 10:18:05.802866 Epoch 104, Training loss 0.21839003853357522\n",
            "2023-06-19 10:18:07.954643 Epoch 105, Training loss 0.21506884860195172\n",
            "2023-06-19 10:18:10.113570 Epoch 106, Training loss 0.21599344961392652\n",
            "2023-06-19 10:18:12.277610 Epoch 107, Training loss 0.2100377691684255\n",
            "2023-06-19 10:18:14.464754 Epoch 108, Training loss 0.20472433294650097\n",
            "2023-06-19 10:18:16.628658 Epoch 109, Training loss 0.20891028637908826\n",
            "2023-06-19 10:18:18.796103 Epoch 110, Training loss 0.20692259832552284\n",
            "2023-06-19 10:18:20.959439 Epoch 111, Training loss 0.20077303858699314\n",
            "2023-06-19 10:18:23.125635 Epoch 112, Training loss 0.197400380092062\n",
            "2023-06-19 10:18:25.306359 Epoch 113, Training loss 0.19492585060133297\n",
            "2023-06-19 10:18:27.487883 Epoch 114, Training loss 0.197190743343086\n",
            "2023-06-19 10:18:29.643169 Epoch 115, Training loss 0.18996138616826883\n",
            "2023-06-19 10:18:31.811833 Epoch 116, Training loss 0.18349518859462374\n",
            "2023-06-19 10:18:33.974224 Epoch 117, Training loss 0.18736180250242257\n",
            "2023-06-19 10:18:36.123108 Epoch 118, Training loss 0.1859385275821777\n",
            "2023-06-19 10:18:38.414164 Epoch 119, Training loss 0.1797355720951299\n",
            "2023-06-19 10:18:40.567715 Epoch 120, Training loss 0.17829990012061064\n",
            "2023-06-19 10:18:42.711260 Epoch 121, Training loss 0.17689884923825597\n",
            "2023-06-19 10:18:44.855107 Epoch 122, Training loss 0.1722654076233791\n",
            "2023-06-19 10:18:47.018097 Epoch 123, Training loss 0.17411991322685957\n",
            "2023-06-19 10:18:49.190781 Epoch 124, Training loss 0.16436843294057119\n",
            "2023-06-19 10:18:51.366254 Epoch 125, Training loss 0.16402549320345472\n",
            "2023-06-19 10:18:53.535180 Epoch 126, Training loss 0.1569426622550199\n",
            "2023-06-19 10:18:55.687796 Epoch 127, Training loss 0.1590914604295591\n",
            "2023-06-19 10:18:57.845992 Epoch 128, Training loss 0.1577925661187263\n",
            "2023-06-19 10:19:00.040054 Epoch 129, Training loss 0.15419667935485293\n",
            "2023-06-19 10:19:02.235043 Epoch 130, Training loss 0.14695630962871442\n",
            "2023-06-19 10:19:04.471880 Epoch 131, Training loss 0.14774521577889752\n",
            "2023-06-19 10:19:06.615526 Epoch 132, Training loss 0.14543309205087127\n",
            "2023-06-19 10:19:08.759377 Epoch 133, Training loss 0.13954640092079046\n",
            "2023-06-19 10:19:10.921619 Epoch 134, Training loss 0.13235606988714\n",
            "2023-06-19 10:19:13.115111 Epoch 135, Training loss 0.13366749541015382\n",
            "2023-06-19 10:19:15.271704 Epoch 136, Training loss 0.12632929865911507\n",
            "2023-06-19 10:19:17.413002 Epoch 137, Training loss 0.12457038300811865\n",
            "2023-06-19 10:19:19.582112 Epoch 138, Training loss 0.12175259015457646\n",
            "2023-06-19 10:19:21.748245 Epoch 139, Training loss 0.12475407984890756\n",
            "2023-06-19 10:19:23.965384 Epoch 140, Training loss 0.1173768861183695\n",
            "2023-06-19 10:19:26.114654 Epoch 141, Training loss 0.11716599968873012\n",
            "2023-06-19 10:19:28.270762 Epoch 142, Training loss 0.11539379723227708\n",
            "2023-06-19 10:19:30.427973 Epoch 143, Training loss 0.11077357176098095\n",
            "2023-06-19 10:19:32.617112 Epoch 144, Training loss 0.112808855760629\n",
            "2023-06-19 10:19:34.803596 Epoch 145, Training loss 0.10878262538344237\n",
            "2023-06-19 10:19:36.971030 Epoch 146, Training loss 0.11051156098960312\n",
            "2023-06-19 10:19:39.126996 Epoch 147, Training loss 0.10080688703022185\n",
            "2023-06-19 10:19:41.285858 Epoch 148, Training loss 0.10282850035341681\n",
            "2023-06-19 10:19:43.446425 Epoch 149, Training loss 0.0943462053539267\n",
            "2023-06-19 10:19:45.660096 Epoch 150, Training loss 0.09671600802451562\n",
            "2023-06-19 10:19:47.853097 Epoch 151, Training loss 0.087927764625686\n",
            "2023-06-19 10:19:50.016030 Epoch 152, Training loss 0.09188202768564224\n",
            "2023-06-19 10:19:52.193808 Epoch 153, Training loss 0.08743468866606427\n",
            "2023-06-19 10:19:54.369002 Epoch 154, Training loss 0.08558677776366662\n",
            "2023-06-19 10:19:56.524498 Epoch 155, Training loss 0.08204102214829177\n",
            "2023-06-19 10:19:58.740927 Epoch 156, Training loss 0.08161352809732127\n",
            "2023-06-19 10:20:00.898966 Epoch 157, Training loss 0.07892521622644108\n",
            "2023-06-19 10:20:03.153446 Epoch 158, Training loss 0.07651951988553925\n",
            "2023-06-19 10:20:05.374612 Epoch 159, Training loss 0.07247902979706503\n",
            "2023-06-19 10:20:07.530736 Epoch 160, Training loss 0.07210354002750223\n",
            "2023-06-19 10:20:09.734455 Epoch 161, Training loss 0.0685032953027707\n",
            "2023-06-19 10:20:11.899441 Epoch 162, Training loss 0.06696926650537807\n",
            "2023-06-19 10:20:14.070672 Epoch 163, Training loss 0.06406901628490846\n",
            "2023-06-19 10:20:16.222444 Epoch 164, Training loss 0.06908081279723507\n",
            "2023-06-19 10:20:18.377919 Epoch 165, Training loss 0.0604850493822318\n",
            "2023-06-19 10:20:20.556018 Epoch 166, Training loss 0.05664976528448284\n",
            "2023-06-19 10:20:22.740846 Epoch 167, Training loss 0.05665325013932529\n",
            "2023-06-19 10:20:24.897191 Epoch 168, Training loss 0.05621202198705476\n",
            "2023-06-19 10:20:27.052576 Epoch 169, Training loss 0.0509371096446256\n",
            "2023-06-19 10:20:29.225099 Epoch 170, Training loss 0.0531101017466681\n",
            "2023-06-19 10:20:31.404043 Epoch 171, Training loss 0.05373793438219341\n",
            "2023-06-19 10:20:33.625677 Epoch 172, Training loss 0.04978324940938289\n",
            "2023-06-19 10:20:35.791239 Epoch 173, Training loss 0.05075689928997664\n",
            "2023-06-19 10:20:37.949012 Epoch 174, Training loss 0.053186143659482335\n",
            "2023-06-19 10:20:40.119045 Epoch 175, Training loss 0.043913725265272104\n",
            "2023-06-19 10:20:42.287821 Epoch 176, Training loss 0.04470986753085237\n",
            "2023-06-19 10:20:44.535950 Epoch 177, Training loss 0.04113255245433112\n",
            "2023-06-19 10:20:46.680880 Epoch 178, Training loss 0.04223305026104875\n",
            "2023-06-19 10:20:48.839800 Epoch 179, Training loss 0.03902741661591894\n",
            "2023-06-19 10:20:50.991774 Epoch 180, Training loss 0.042775651155288814\n",
            "2023-06-19 10:20:53.167559 Epoch 181, Training loss 0.036901564337313175\n",
            "2023-06-19 10:20:55.391281 Epoch 182, Training loss 0.040382969021132795\n",
            "2023-06-19 10:20:57.573453 Epoch 183, Training loss 0.03827432782098556\n",
            "2023-06-19 10:20:59.734754 Epoch 184, Training loss 0.03883269893681737\n",
            "2023-06-19 10:21:01.928746 Epoch 185, Training loss 0.03916344593818875\n",
            "2023-06-19 10:21:04.202037 Epoch 186, Training loss 0.03388651359565319\n",
            "2023-06-19 10:21:06.416150 Epoch 187, Training loss 0.032081180560598326\n",
            "2023-06-19 10:21:08.617019 Epoch 188, Training loss 0.032322753552988075\n",
            "2023-06-19 10:21:10.798402 Epoch 189, Training loss 0.030886749481889093\n",
            "2023-06-19 10:21:12.981680 Epoch 190, Training loss 0.03509355268209793\n",
            "2023-06-19 10:21:15.157478 Epoch 191, Training loss 0.0407951644259938\n",
            "2023-06-19 10:21:17.353243 Epoch 192, Training loss 0.031118158654422516\n",
            "2023-06-19 10:21:19.581426 Epoch 193, Training loss 0.028051751197143725\n",
            "2023-06-19 10:21:21.758702 Epoch 194, Training loss 0.03314907151232859\n",
            "2023-06-19 10:21:23.932156 Epoch 195, Training loss 0.02952864613668744\n",
            "2023-06-19 10:21:26.116412 Epoch 196, Training loss 0.02676063983932517\n",
            "2023-06-19 10:21:28.286895 Epoch 197, Training loss 0.026417309412388665\n",
            "2023-06-19 10:21:30.483888 Epoch 198, Training loss 0.02495878315263778\n",
            "2023-06-19 10:21:32.633113 Epoch 199, Training loss 0.025096622236357752\n",
            "2023-06-19 10:21:34.802549 Epoch 200, Training loss 0.028717100694419663\n",
            "Accuracy train: 0.99\n",
            "Accuracy val: 0.89\n",
            "Number of free parameters in the model: 1531090\n"
          ]
        }
      ]
    }
  ]
}